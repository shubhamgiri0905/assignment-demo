{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM89SQThk1uVLFhmIxaWe7s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhamgiri0905/assignment-demo/blob/main/ContinualLearningExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbgPLTQ-L7lm",
        "outputId": "5a92d606-e32f-4f90-8efd-895582f3fc33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Documentation for Catastrophic Forgetting and Continual Learning**\n",
        "\n",
        "This documentation explains the concept of **Catastrophic Forgetting** and how it can be tackled using **Continual Learning**. We will break down the theory and provide a detailed explanation of the code used to demonstrate these concepts. Even for non-technical readers, the document will cover the core ideas in a way that is easy to understand.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. What is Catastrophic Forgetting?**\n",
        "\n",
        "**Catastrophic Forgetting** refers to the phenomenon in which a neural network forgets previously learned information when trained on new data. Imagine that a neural network learns to recognize digits 0-4 (Task A). Once it starts learning digits 5-9 (Task B), it might perform very well on Task B but completely forget what it learned about Task A (digits 0-4). This happens because neural networks typically adjust their weights during training, and new information overwrites the learned knowledge, causing previous knowledge to be lost.\n",
        "\n",
        "### **2. What is Continual Learning?**\n",
        "\n",
        "**Continual Learning** is an approach used to address the problem of Catastrophic Forgetting. The idea is to allow a model to learn continuously from new tasks without forgetting previously learned tasks. One of the most popular methods to achieve this is by **replaying** past examples (or even using specialized algorithms that regularize learning to avoid forgetting).\n",
        "\n",
        "In this document, we demonstrate a basic example of Catastrophic Forgetting using the MNIST dataset and a simple neural network (CNN), and we show how Continual Learning using **Naïve Replay** can help mitigate this issue.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Overview of the Code**\n",
        "\n",
        "The provided Python code does the following:\n",
        "\n",
        "1. **Demonstrates Catastrophic Forgetting**: We train a neural network on Task A (digits 0–4), then continue training on Task B (digits 5–9). We then show how the accuracy on Task A drops as the model learns Task B, demonstrating **Catastrophic Forgetting**.\n",
        "\n",
        "2. **Introduces Continual Learning**: We introduce a simple **Naïve Replay** strategy, where we store a small number of samples from Task A in a **replay buffer**. When training on Task B, we mix the replayed samples from Task A with the current data from Task B. This helps the model \"remember\" Task A while learning Task B.\n",
        "\n",
        "Let’s break down the core sections of the code:\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Data Preparation (Task A & Task B)**\n",
        "\n",
        "We use the **MNIST dataset**, which consists of images of handwritten digits (0-9). The task is to train the neural network on two separate groups of digits (0–4 for Task A and 5–9 for Task B).\n",
        "\n",
        "* **Task A**: Digits 0, 1, 2, 3, 4\n",
        "* **Task B**: Digits 5, 6, 7, 8, 9\n",
        "\n",
        "#### **Why do we separate the data like this?**\n",
        "\n",
        "By dividing the MNIST data into two tasks, we can simulate a situation where the neural network has to learn one set of data (Task A) and then learn a completely new set of data (Task B) without forgetting the first set. This demonstrates how catastrophic forgetting happens.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Model Architecture**\n",
        "\n",
        "We define a simple **Convolutional Neural Network (CNN)** using PyTorch. The CNN has the following components:\n",
        "\n",
        "1. **Convolutional Layers**: These layers learn spatial patterns in images (e.g., edges, corners, shapes).\n",
        "2. **MaxPool Layers**: These reduce the dimensionality of the images, helping the network focus on important features.\n",
        "3. **Fully Connected Layers**: These are the final layers that output class scores for each digit (0-9).\n",
        "\n",
        "#### **Why CNN?**\n",
        "\n",
        "CNNs are commonly used in image classification tasks because they are very efficient at learning patterns from images, such as edges and textures, which are important for recognizing digits in the MNIST dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Training the Model on Task A and Task B**\n",
        "\n",
        "* **Task A Training**: We first train the model on digits 0–4 (Task A). During this phase, the model learns to recognize the patterns in these digits.\n",
        "\n",
        "* **Task B Training**: After the model is trained on Task A, we then continue training it on digits 5–9 (Task B). **Catastrophic Forgetting** happens at this stage, and the model may lose its ability to correctly classify Task A digits.\n",
        "\n",
        "#### **Why sequential training?**\n",
        "\n",
        "Sequential training simulates a real-world scenario where models often need to learn new tasks over time. For example, a model that learns to identify objects in one domain might later need to learn to recognize objects in a completely different domain.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Evaluating the Model for Forgetting**\n",
        "\n",
        "After training on Task A and then on Task B, we evaluate the model on Task A again. We compare the accuracy before and after training on Task B:\n",
        "\n",
        "* **Before Task B Training**: The model’s accuracy on Task A is high because it has just finished learning Task A.\n",
        "* **After Task B Training**: The model’s accuracy on Task A decreases, demonstrating **Catastrophic Forgetting**.\n",
        "\n",
        "#### **Why do we evaluate again?**\n",
        "\n",
        "We want to observe how well the model retains knowledge of Task A after being trained on Task B. The difference in accuracy shows the impact of catastrophic forgetting.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Introducing Naïve Replay for Continual Learning**\n",
        "\n",
        "We implement a **Naïve Replay** strategy to tackle Catastrophic Forgetting:\n",
        "\n",
        "1. **Replay Buffer**: We randomly store 200 samples from Task A in a buffer.\n",
        "2. **Replay Training**: When training on Task B, we mix the stored Task A samples with the new Task B samples. This ensures the model retains some memory of Task A while learning Task B.\n",
        "\n",
        "#### **Why Naïve Replay?**\n",
        "\n",
        "Naïve Replay is a simple and effective technique. By re-exposing the model to examples from previous tasks, we prevent it from forgetting what it has learned. It simulates how human memory works: by constantly recalling past experiences while learning new ones.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Final Evaluation and Comparison**\n",
        "\n",
        "We compare the model’s performance:\n",
        "\n",
        "* **Task A Before Task B Training**: High accuracy.\n",
        "* **Task A After Task B Training (No Replay)**: Decreased accuracy, demonstrating catastrophic forgetting.\n",
        "* **Task A After Task B Training (With Replay)**: The accuracy improves, showing that replay helps mitigate forgetting.\n",
        "\n",
        "#### **Why is this comparison important?**\n",
        "\n",
        "It clearly demonstrates the effectiveness of Continual Learning with Naïve Replay in preventing catastrophic forgetting. By using a simple buffer of previous examples, the model can retain knowledge across multiple tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Conclusion**\n",
        "\n",
        "**Catastrophic Forgetting** is a major challenge in machine learning, especially when dealing with sequential tasks. However, using techniques like **Naïve Replay**, where past examples are replayed during training on new tasks, we can significantly reduce the impact of forgetting.\n",
        "\n",
        "This code serves as a basic demonstration of how this phenomenon works and how it can be mitigated. The model’s performance improves after incorporating replay, demonstrating that even simple solutions can help overcome catastrophic forgetting in a continual learning setup.\n",
        "\n",
        "---\n",
        "\n",
        "### **11. Real Case Scenario To Relate**\n",
        "\n",
        "Imagine you are using a recommendation system on an e-commerce website. Over time, the system needs to learn your preferences for different types of products. If the system learns about new products but forgets the old ones, it could start recommending irrelevant items. The replay strategy in this code ensures that the system continues to recommend both new and old products effectively, helping it “remember” past preferences while incorporating new ones.\n",
        "\n",
        "This type of continual learning ensures that machines can keep learning without losing important information, which is essential for real-world applications such as customer recommendations, autonomous driving, and healthcare monitoring systems.\n",
        "\n"
      ],
      "metadata": {
        "id": "1bTTOPaWPDCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# continual_forgetting_demo.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Hyperparameters & Setup\n",
        "# ----------------------------\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "replay_buffer_size = 200    # number of samples to store from Task A\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Data Preparation\n",
        "# ----------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "full_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "full_test  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Task A: digits 0–4\n",
        "idx_a_train = [i for i, (_, y) in enumerate(full_train) if y < 5]\n",
        "idx_a_test  = [i for i, (_, y) in enumerate(full_test)  if y < 5]\n",
        "taskA_train = Subset(full_train, idx_a_train)\n",
        "taskA_test  = Subset(full_test,  idx_a_test)\n",
        "\n",
        "# Task B: digits 5–9\n",
        "idx_b_train = [i for i, (_, y) in enumerate(full_train) if y >= 5]\n",
        "idx_b_test  = [i for i, (_, y) in enumerate(full_test)  if y >= 5]\n",
        "taskB_train = Subset(full_train, idx_b_train)\n",
        "taskB_test  = Subset(full_test,  idx_b_test)\n",
        "\n",
        "loader_A_train = DataLoader(taskA_train, batch_size=batch_size, shuffle=True)\n",
        "loader_A_test  = DataLoader(taskA_test,  batch_size=batch_size, shuffle=False)\n",
        "loader_B_train = DataLoader(taskB_train, batch_size=batch_size, shuffle=True)\n",
        "loader_B_test  = DataLoader(taskB_test,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Model Definition\n",
        "# ----------------------------\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, 1, 1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, 3,1,1),  nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32*7*7, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, num_classes),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "def train(model, optimizer, criterion, loader):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            preds = out.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Phase I: Sequential Training (Shows Forgetting)\n",
        "# ----------------------------\n",
        "model = SimpleCNN().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\n== Training on Task A (digits 0–4) ==\")\n",
        "for ep in range(1, epochs+1):\n",
        "    loss = train(model, optimizer, criterion, loader_A_train)\n",
        "    acc  = evaluate(model, loader_A_test)\n",
        "    print(f\"Epoch {ep}/{epochs}  Loss: {loss:.4f}  TaskA Acc: {acc:.2f}%\")\n",
        "\n",
        "print(\"\\n== Evaluate on Task A BEFORE Task B Training ==\")\n",
        "acc_A_before = evaluate(model, loader_A_test)\n",
        "print(f\"Accuracy on Task A: {acc_A_before:.2f}%\")\n",
        "\n",
        "print(\"\\n== Continue Training on Task B (digits 5–9) ==\")\n",
        "for ep in range(1, epochs+1):\n",
        "    loss = train(model, optimizer, criterion, loader_B_train)\n",
        "    accB = evaluate(model, loader_B_test)\n",
        "    print(f\"Epoch {ep}/{epochs}  Loss: {loss:.4f}  TaskB Acc: {accB:.2f}%\")\n",
        "\n",
        "print(\"\\n== Evaluate on Task A AFTER Task B Training ==\")\n",
        "acc_A_after = evaluate(model, loader_A_test)\n",
        "print(f\"Accuracy on Task A: {acc_A_after:.2f}%\")\n",
        "print(f\"Catastrophic forgetting: ΔAcc = {acc_A_before - acc_A_after:.2f}%\")\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Phase II: Continual Learning with Naïve Replay\n",
        "# ----------------------------\n",
        "print(\"\\n\\n== Continual Learning: Replay Buffer ==\")\n",
        "# 5.1 Build replay buffer from Task A\n",
        "buffer_indices = random.sample(idx_a_train, replay_buffer_size)\n",
        "replay_buffer = Subset(full_train, buffer_indices)\n",
        "replay_loader = DataLoader(replay_buffer, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 5.2 New model for fair comparison\n",
        "model_replay = SimpleCNN().to(device)\n",
        "optimizer_replay = optim.SGD(model_replay.parameters(), lr=0.01)\n",
        "\n",
        "# 5.3 Train Task A (again)\n",
        "for ep in range(1, epochs+1):\n",
        "    train(model_replay, optimizer_replay, criterion, loader_A_train)\n",
        "\n",
        "# 5.4 Joint training on Task B + replayed Task A\n",
        "joint_loader = DataLoader(\n",
        "    ConcatDataset([replay_buffer, taskB_train]),\n",
        "    batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "for ep in range(1, epochs+1):\n",
        "    loss = train(model_replay, optimizer_replay, criterion, joint_loader)\n",
        "    accB = evaluate(model_replay, loader_B_test)\n",
        "    print(f\"Epoch {ep}/{epochs}  Loss: {loss:.4f}  TaskB Acc: {accB:.2f}%\")\n",
        "\n",
        "# 5.5 Evaluate on Task A again\n",
        "acc_A_replay = evaluate(model_replay, loader_A_test)\n",
        "print(f\"\\nPost-replay Accuracy on Task A: {acc_A_replay:.2f}%\")\n",
        "print(f\"Reduced forgetting: ΔAcc = {acc_A_before - acc_A_replay:.2f}% (vs {acc_A_before - acc_A_after:.2f}% before)\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Final Summary\n",
        "# ----------------------------\n",
        "print(\"\\nSummary:\")\n",
        "print(f\"- TaskA before TaskB: {acc_A_before:.2f}%\")\n",
        "print(f\"- TaskA after TaskB (no replay): {acc_A_after:.2f}%\")\n",
        "print(f\"- TaskA after TaskB (with replay): {acc_A_replay:.2f}%\")\n",
        "print(\"You can see how a small replay buffer helps mitigate catastrophic forgetting.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDPxSQzyMqNO",
        "outputId": "8b48a5aa-1db6-4616-c5ab-408c8cb2101d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 499kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.56MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.99MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== Training on Task A (digits 0–4) ==\n",
            "Epoch 1/5  Loss: 0.3664  TaskA Acc: 97.31%\n",
            "Epoch 2/5  Loss: 0.1075  TaskA Acc: 98.31%\n",
            "Epoch 3/5  Loss: 0.0833  TaskA Acc: 98.15%\n",
            "Epoch 4/5  Loss: 0.0691  TaskA Acc: 98.46%\n",
            "Epoch 5/5  Loss: 0.0571  TaskA Acc: 98.99%\n",
            "\n",
            "== Evaluate on Task A BEFORE Task B Training ==\n",
            "Accuracy on Task A: 98.99%\n",
            "\n",
            "== Continue Training on Task B (digits 5–9) ==\n",
            "Epoch 1/5  Loss: 0.3037  TaskB Acc: 95.64%\n",
            "Epoch 2/5  Loss: 0.1054  TaskB Acc: 96.56%\n",
            "Epoch 3/5  Loss: 0.0781  TaskB Acc: 97.80%\n",
            "Epoch 4/5  Loss: 0.0637  TaskB Acc: 96.81%\n",
            "Epoch 5/5  Loss: 0.0545  TaskB Acc: 98.50%\n",
            "\n",
            "== Evaluate on Task A AFTER Task B Training ==\n",
            "Accuracy on Task A: 0.35%\n",
            "Catastrophic forgetting: ΔAcc = 98.64%\n",
            "\n",
            "\n",
            "== Continual Learning: Replay Buffer ==\n",
            "Epoch 1/5  Loss: 0.3365  TaskB Acc: 95.41%\n",
            "Epoch 2/5  Loss: 0.1352  TaskB Acc: 96.40%\n",
            "Epoch 3/5  Loss: 0.1012  TaskB Acc: 96.05%\n",
            "Epoch 4/5  Loss: 0.0829  TaskB Acc: 97.57%\n",
            "Epoch 5/5  Loss: 0.0725  TaskB Acc: 98.07%\n",
            "\n",
            "Post-replay Accuracy on Task A: 70.81%\n",
            "Reduced forgetting: ΔAcc = 28.18% (vs 98.64% before)\n",
            "\n",
            "Summary:\n",
            "- TaskA before TaskB: 98.99%\n",
            "- TaskA after TaskB (no replay): 0.35%\n",
            "- TaskA after TaskB (with replay): 70.81%\n",
            "You can see how a small replay buffer helps mitigate catastrophic forgetting.\n"
          ]
        }
      ]
    }
  ]
}